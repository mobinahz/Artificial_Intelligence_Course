{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression"
      ],
      "metadata": {
        "id": "D6o5N2vfjkAN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main form of simple linear regression function:\n",
        "$$f(x) = \\alpha x + \\beta$$\n",
        "\n",
        "here we want to find the bias ($\\alpha$) and slope($\\beta$) by minimizing the derivation of the Residual Sum of Squares (RSS) function:\n",
        "\n",
        "- step 1: Compute RSS of the training data  \n",
        "\n",
        "$$ RSS = \\Sigma (y_i - (\\hat{\\beta} + \\hat{\\alpha} * x_i) )^2 $$\n",
        "\n",
        "- step 2: Compute the derivatives of the RSS function in terms of $\\alpha$ and $\\beta$, and set them equal to 0 to find the desired parameters\n",
        "\n",
        "$$ \\frac{\\partial RSS}{\\partial \\beta} = \\Sigma (-f(x_i) + \\hat{\\beta} + \\hat{\\alpha} * x_i) = 0$$\n",
        "$$ \\to \\beta = \\hat{y} - \\hat{\\alpha} \\hat{x} \\to (1)$$\n",
        "\n",
        "\n",
        "$$ \\frac{\\partial RSS}{\\partial \\alpha} = \\Sigma (-2 x_i y_i + 2 \\hat{\\beta} x_i + 2\\hat{\\alpha} x_i ^ 2) = 0 \\to (2)$$\n",
        "\n",
        "$$ (1) , (2) \\to \\hat{\\alpha} = \\frac{\\Sigma{(x_i - \\hat{x})(y_i - \\hat{y})}}{\\Sigma{(x_i - \\hat{x})^2}}\n",
        "$$\n",
        "$$ \\hat{\\beta} = y - \\hat{a} x$$\n",
        "\n"
      ],
      "metadata": {
        "id": "uOZ_t3DjjnQz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the above formula, implement the function below to compute the parameters of a simple linear regression"
      ],
      "metadata": {
        "id": "PosEWpa7j1Z8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_regression(input, output):\n",
        "  #TO DO\n",
        "  pass"
      ],
      "metadata": {
        "id": "tgq8tHPXjkkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now complete this `get_regression_predictions(...)` function to predict the value of given data based on the calculated intercept and slope"
      ],
      "metadata": {
        "id": "CL5v360zklZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_regression_predictions(input, intercept, slope):\n",
        "    #TO DO\n",
        "    pass"
      ],
      "metadata": {
        "id": "2cq1rghRkhKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a model and can make predictions, let's evaluate our model using Root Mean Square Error (RMSE). RMSE is the square root of the mean of the squared differences between the residuals, and the residuals is just a fancy word for the difference between the predicted output and the true output.\n",
        "\n",
        "Complete the following function to compute the RSME of a simple linear regression model given the input_feature, output, intercept and slope:"
      ],
      "metadata": {
        "id": "HwXzz8dDkv7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_root_mean_square_error(predicted_values, actual_values):\n",
        "  #TO DO\n",
        "  pass\n"
      ],
      "metadata": {
        "id": "nujfA_y3kves"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The RMSE has no bound, thus it becomes challenging to determine whether a particular RMSE value is considered good or bad without any reference point. Instead, we use R2 score. The R2 score is calculated by comparing the sum of the squared differences between the actual and predicted values of the dependent variable to the total sum of squared differences between the actual and mean values of the dependent variable. The R2 score is formulated as below:\n",
        "\n",
        "$$R^2 = 1 - \\frac{SSres}{SStot} = 1 - \\frac{\\sum_{i=1}^{n} (y_{i,true} - y_{i,pred})^2}{\\sum_{i=1}^{n} (y_{i,true} - \\bar{y}_{true})^2} $$"
      ],
      "metadata": {
        "id": "ByK1k1X1k1YE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete the following function to calculate the R2 score of a given input_feature, output, bias, and slope:"
      ],
      "metadata": {
        "id": "B70NqPIkk7P0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_r2_score(predicted_values, actual_values):\n",
        "  #TO DO\n",
        "  pass\n"
      ],
      "metadata": {
        "id": "rRFf4yqmk9Jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now calculate the fitness of the model.\n",
        "Remember to provide explanation for the outputs in your code!"
      ],
      "metadata": {
        "id": "HxOOUjmvk_bT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "designated_feature_list = [] # ToDo\n",
        "\n",
        "for feature in designated_feature_list:\n",
        "  pass\n",
        "  #TO DO"
      ],
      "metadata": {
        "id": "S38EwioclBiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ploynomial Regression"
      ],
      "metadata": {
        "id": "b5xJaLcqkYNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To extend the simple linear regression to polynomial regression, we can model the relationship between the independent variable $x$ and the dependent variable $y$ as a polynomial function of degree $n$:\n",
        "\n",
        "$$f(x) = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\ldots + \\beta_nx^n$$\n",
        "\n",
        "The steps to find the parameters $\\beta_i$ are similar to those in simple linear regression. We again minimize the RSS function by taking the derivatives with respect to each parameter and setting them to 0.\n",
        "\n",
        "- Step 1: Compute the RSS function for polynomial regression:\n",
        "\n",
        "$$ RSS = \\Sigma (y_i - (\\hat{\\beta_0} + \\hat{\\beta_1}x_i + \\hat{\\beta_2}x_i^2 + \\ldots + \\hat{\\beta_n}x_i^n))^2 $$\n",
        "\n",
        "- Step 2: Compute the derivatives of the RSS function with respect to each parameter $\\beta_i$ and set them to 0 to find the desired parameters.\n",
        "\n",
        "$$ \\frac{\\partial RSS}{\\partial \\beta_i} = 0, \\text{ for } i = 0, 1, 2, \\ldots, n$$\n",
        "\n",
        "Solving these equations will give us the optimal values of $\\beta_i$ for the polynomial regression model. The specific form of the equations will depend on the degree of the polynomial and the number of parameters.\n",
        "\n",
        "The general form for finding the coefficients for polynomial regression can be represented as:\n",
        "\n",
        "$$ \\beta = (X^T X)^{-1} X^T y $$\n",
        "\n",
        "where:\n",
        "- $X$ is the design matrix with columns $x^0, x^1, x^2, ..., x^n$\n",
        "- $x^i$ represents the feature vector of $x$ raised to the power of $i$\n",
        "- $y$ is the target variable vector\n",
        "- $\\beta$ is the coefficient vector for the polynomial regression\n",
        "\n",
        "By solving for $\\beta$ using the above formula, we can obtain the coefficients for the polynomial regression model."
      ],
      "metadata": {
        "id": "wkVMCH41kLdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def polynomial_regression(x, y, degree):\n",
        "  pass\n",
        "  #TO DO"
      ],
      "metadata": {
        "id": "Z4LNTn2tkMHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing the Derivative\n",
        "\n",
        "As we saw, the cost function is the sum over the data points of the squared difference between an observed output and a predicted output.\n",
        "\n",
        "Since the derivative of a sum is the sum of the derivatives, we can compute the derivative for a single data point and then sum over data points. We can write the squared difference between the observed output and predicted output for a single point as follows:\n",
        "\n",
        "$$\n",
        "(output  - (const* w _{0} + [feature_1] * w_{1} + ...+ [feature_n] * w_{n}  ))^2\n",
        "$$\n",
        "\n",
        "With n feautures and a const , So the derivative will be :\n",
        "\n",
        "\n",
        "$$\n",
        "2 * (output  - (const* w _{0} + [feature_1] * w_{1} + ...+ [feature_n] * w_{n}  ))\n",
        "$$\n",
        "\n",
        "The term inside the paranethesis is just the error (difference between prediction and output). So we can re-write this as:\n",
        "\n",
        "$$2 * error*[feature_i] $$\n",
        "\n",
        "\n",
        "That is, the derivative for the weight for feature i is the sum (over data points) of 2 times the product of the error and the feature itself. In the case of the constant then this is just twice the sum of the errors!\n",
        "\n",
        "Recall that twice the sum of the product of two vectors is just twice the dot product of the two vectors. Therefore the derivative for the weight for feature_i is just two times the dot product between the values of feature_i and the current errors.\n",
        "\n",
        "\n",
        "With this in mind, complete the following derivative function which computes the derivative of the weight given the value of the feature (over all data points) and the errors (over all data points).\n"
      ],
      "metadata": {
        "id": "JqETl66ong-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_derivative(errors, feature):\n",
        "  #TO DO\n",
        "  pass"
      ],
      "metadata": {
        "id": "bcfkt5wZnrIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent\n",
        "\n",
        "Now we will write a function that performs a gradient descent. The basic premise is simple. Given a starting point we update the current weights by moving in the negative gradient direction. Recall that the gradient is the direction of increase and therefore the negative gradient is the direction of decrease and we're trying to minimize a cost function.\n",
        "\n",
        "\n",
        "The amount by which we move in the negative gradient direction is called the 'step size'. We stop when we are 'sufficiently close' to the optimum. We define this by requiring that the magnitude (length) of the gradient vector to be smaller than a fixed 'tolerance'.\n",
        "\n",
        "\n",
        "With this in mind, complete the following gradient descent function below using your derivative function above. For each step in the gradient descent we update the weight for each feature befofe computing our stopping criteria."
      ],
      "metadata": {
        "id": "1pyRt-sBlVDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility functions for multiple regression\n",
        "\n",
        "def normalize_features(chosen_features, data_frame):\n",
        "    for feature in chosen_features:\n",
        "        data_frame[feature] = (data_frame[feature] - data_frame[feature].mean()) / data_frame[feature].std()\n",
        "    return data_frame\n",
        "\n",
        "def predict_output(feature_matrix, weights, bias):\n",
        "    #TO DO FOR POLYNOMIAL REGRESSION PREDICTION\n",
        "    return predictions\n",
        "\n"
      ],
      "metadata": {
        "id": "w4das-ABlaaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Polynomial Regression Using Gradient Descent\n",
        "\n",
        "Polynomial regression using gradient descent involves finding the optimal parameters for a polynomial model by iteratively updating them based on the gradient of a loss function, typically the Mean Squared Error (MSE). The steps involved are as follows:\n",
        "\n",
        "- **Step 1: Define the polynomial model**\n",
        "The polynomial model has the form:\n",
        "$$f(x) = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\ldots + \\beta_nx^n$$\n",
        "\n",
        "- **Step 2: Define the loss function**\n",
        "The loss function, such as Mean Squared Error (MSE), measures the error between the actual target values and the predicted values by the model.\n",
        "\n",
        "- **Step 3: Initialize the coefficients**\n",
        "Start with initial guesses for the coefficients $\\beta_0, \\beta_1, \\ldots, \\beta_n$\n",
        "\n",
        "- **Step 4: Update the coefficients using Gradient Descent**\n",
        "Iteratively update the coefficients to minimize the loss function. This is done by computing the gradient of the loss function with respect to each coefficient and making small adjustments in the opposite direction of the gradient.\n",
        "\n",
        "- **Step 5: Repeat until convergence**\n",
        "Continue updating the coefficients iteratively until the algorithm converges to the optimal values.\n",
        "\n",
        "- **Step 6: Use the learned coefficients for prediction**\n",
        "Once the coefficients converge, they can be used in the polynomial function to make predictions on new data points.\n",
        "\n",
        "Overall, polynomial regression using gradient descent is an iterative optimization process that aims to find the best-fitting polynomial curve to the data points by minimizing the prediction errors. The learning rate and the number of iterations are key hyperparameters to tune for efficient convergence and accurate modeling.\n",
        "\n"
      ],
      "metadata": {
        "id": "divz9_fEmjma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def polynomial_regression_gradient_descent(feature_matrix, outputs, initial_weights,bias, step_size, tolerance):\n",
        "    weights = np.array(initial_weights)\n",
        "\n",
        "    while True:\n",
        "        # Compute predictions using polynomial function and errors\n",
        "        #TO DO\n",
        "\n",
        "        # Compute derivatives for all weights\n",
        "        #TO DO\n",
        "\n",
        "        # Update weights and bias\n",
        "        #TO DO\n",
        "\n",
        "        # Check convergence\n",
        "        #TO DO\n",
        "\n",
        "    return weights, bias"
      ],
      "metadata": {
        "id": "1EdzTgrzlWlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def run_polynomial_regression(chosen_feature_matrix, target_matrix, keywords):\n",
        "    initial_weights = keywords['initial_weights']\n",
        "    step_size = keywords['step_size']\n",
        "    tolerance = keywords['tolerance']\n",
        "    bias = keywords['bias']\n",
        "    weights = np.array(initial_weights)\n",
        "    weights, bias = polynomial_regression_gradient_descent(chosen_feature_matrix, target_matrix, weights, bias, step_size, tolerance)\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "def get_weights_and_bias(chosen_features):\n",
        "\n",
        "    keywords = {\n",
        "        'initial_weights': np.array([.5]*len(chosen_features)),\n",
        "        'step_size': 1.e-4,\n",
        "        'tolerance': 1.e-10,\n",
        "        'bias': 0\n",
        "    }\n",
        "\n",
        "    # TO DO\n",
        "\n",
        "    return chosen_feature_matrix, train_weights, bias"
      ],
      "metadata": {
        "id": "1QAqzvnEltkW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}